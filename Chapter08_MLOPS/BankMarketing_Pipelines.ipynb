{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from azureml.core import Dataset, Run\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score #metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# sklearn.externals.joblib is removed in 0.23\n",
    "from sklearn import __version__ as sklearnver\n",
    "from packaging.version import Version\n",
    "if Version(sklearnver) < Version(\"0.23.0\"):\n",
    "    from sklearn.externals import joblib\n",
    "else:\n",
    "    import joblib\n",
    "    \n",
    "run = Run.get_context()\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.2,\n",
    "                        help='learning_rate parameter to be used in the algorithm')\n",
    "    parser.add_argument('--n_estimators', type=int, default=100,\n",
    "                        help='n_estimators to be used in the algorithm')\n",
    "    parser.add_argument('--max_depth', type=int, default=3,\n",
    "                        help='max_depth parameter to be used in the algorithm')\n",
    "    parser.add_argument('--min_samples_split', type=int, default=100,\n",
    "                        help='min_samples_split to be used in the algorithm')\n",
    "    parser.add_argument('--min_samples_leaf', type=int, default=100,\n",
    "                        help='min_samples_leaf to be used in the algorithm')\n",
    "    parser.add_argument('--subsample', type=float, default=3,\n",
    "                        help='subsample parameter to be used in the algorithm')\n",
    "    parser.add_argument('--random_state', type=int, default=0.7,\n",
    "                        help='random_state to be used in the algorithm')\n",
    "    parser.add_argument('--max_features', type=int, default=0.0,\n",
    "                        help='max_features parameter to be used in the algorithm')\n",
    "    \n",
    "\n",
    "    args = parser.parse_args()  \n",
    "    run.log('learning_rate', np.float(args.learning_rate))\n",
    "    run.log('n_estimators', np.int(args.n_estimators))\n",
    "    run.log('max_depth', np.int(args.max_depth))\n",
    "    run.log('min_samples_split', np.int(args.min_samples_split))\n",
    "    run.log('min_samples_leaf', np.int(args.min_samples_leaf))\n",
    "    run.log('subsample', np.float(args.subsample))\n",
    "    run.log('random_state', np.int(args.subsample))\n",
    "    run.log('max_features', np.int(args.subsample))\n",
    "\n",
    "\n",
    "    # get input dataset by name\n",
    "    bank_dataset = run.input_datasets['bank_dataset']\n",
    "    data = bank_dataset.to_pandas_dataframe()\n",
    "\n",
    "    \n",
    "    # Data Cleaning\n",
    "    cat_col = ['default', 'housing', 'loan', 'deposit', 'job', \n",
    "                'marital', 'education', 'contact', 'month', 'poutcome']\n",
    "    for column in cat_col:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(data[column])\n",
    "        label_encoded_y = label_encoder.transform(data[column])\n",
    "        data[column + '_cat'] = label_encoded_y\n",
    "    #     data[column + '_bool'] = data[column].eq('yes').mul(1)\n",
    "    data = data.drop(columns = cat_col)\n",
    "    \n",
    "    #drop irrelevant columns\n",
    "    data = data.drop(columns = ['pdays'])\n",
    "    #impute incorrect values and drop original columns\n",
    "    def get_correct_values(row, column_name, threshold, df):\n",
    "        ''' Returns mean value if value in column_name is above threshold'''\n",
    "        if row[column_name] <= threshold:\n",
    "            return row[column_name]\n",
    "        else:\n",
    "            mean = df[df[column_name] <= threshold][column_name].mean()\n",
    "            return mean\n",
    "    data['campaign_cleaned'] = data.apply(lambda row: get_correct_values(row, 'campaign', 50, data),axis=1)\n",
    "    data['previous_cleaned'] = data.apply(lambda row: get_correct_values(row, 'previous', 50, data),axis=1)\n",
    "    data = data.drop(columns = ['campaign', 'previous'])\n",
    "\n",
    "\n",
    "    # Model Training\n",
    "    X = data.drop(columns = 'deposit_cat')\n",
    "    y = data[['deposit_cat']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "    Params = {'learning_rate': np.float(args.learning_rate),\n",
    "              'n_estimators': np.int(args.n_estimators),\n",
    "              'max_depth': np.int(args.max_depth),\n",
    "              'min_samples_split': np.int(args.min_samples_split),\n",
    "              'min_samples_leaf': np.int(args.min_samples_leaf),\n",
    "              'subsample': np.float(args.subsample),\n",
    "              'random_state': np.int(args.random_state),\n",
    "              'max_features': np.int(args.max_features)}\n",
    "        \n",
    "    # GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(**Params)\n",
    "    clf.fit(X_train,y_train.squeeze().values)\n",
    "    \n",
    "    #calculate and print scores for the model \n",
    "    y_train_preds = clf.predict(X_train)\n",
    "    y_test_preds = clf.predict(X_test)\n",
    "\n",
    "\n",
    "    model_file_name = 'joblibGB_bankmarketing.sav'\n",
    "\n",
    "    accuracy_score_train = accuracy_score(y_train, y_train_preds)\n",
    "    accuracy_score_test = accuracy_score(y_test, y_test_preds)\n",
    "    run.log('Gradient Boosting Accuracy Score for training', accuracy_score_train)\n",
    "    run.log('Graident Boosting Accuracy Score for testing', accuracy_score_test)\n",
    "\n",
    "    # Save the trained model\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(value=clf, filename='outputs/' + model_file_name)    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    " \n",
    "from azureml.core import Dataset\n",
    " \n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    " \n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    " \n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    " \n",
    "from azureml.core.model import Model\n",
    " \n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "aml_compute = \"ninjacpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=aml_compute)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "aml_run_config.environment.docker.base_image = \"mcr.microsoft.com/azureml/base:0.2.1\"\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-dataset-runtime[fuse]', 'packaging', 'numpy==1.16.2'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = Datastore.get(ws, \"kaggledatabook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "bank_dataset = Dataset.Tabular.from_delimited_files(path=blob_datastore.path('Bank.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanse script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/adscompute/code/Users/prsing/BankMarketingAnalysis/ML_Pipelines/scripts/prepdata.\n",
      "cleansingStep created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_data = PipelineData(\"cleansed_data\", datastore=blob_datastore).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStep = PythonScriptStep(\n",
    "    name=\"Cleanse Bank Marketing Data\",\n",
    "    script_name=\"prep.py\", \n",
    "    arguments=[\"--output_cleanse\", cleansed_data],\n",
    "    inputs=[bank_dataset.as_named_input('bank_dataset')],\n",
    "    outputs=[cleansed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStep created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "trainmodel = PythonScriptStep(name=\"train_step\",\n",
    "                         script_name=\"./train.py\", \n",
    "                         arguments=[\"--train\", train_data,\"--test\", test_data,\"--model\",model_file],\n",
    "                         inputs= [cleansed_data.parse_parquet_files(file_extension=None)],\n",
    "                         outputs=[model_file],                         \n",
    "                         compute_target=aml_compute, \n",
    "                         runconfig=aml_run_config,\n",
    "                         source_directory=train_model_folder,\n",
    "                         allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda Civic</td>\n",
       "      <td>22000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toyota Corolla</td>\n",
       "      <td>25000</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford Focus</td>\n",
       "      <td>27000</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audi A4</td>\n",
       "      <td>35000</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand  Price  check\n",
       "0     Honda Civic  22000     22\n",
       "1  Toyota Corolla  25000     33\n",
       "2      Ford Focus  27000     44\n",
       "3         Audi A4  35000     66"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000],\n",
    "        'check': [22, 33,44, 66]\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Brand', 'Price', 'check'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Honda Civic', 22000, 22],\n",
       "       ['Toyota Corolla', 25000, 33],\n",
       "       ['Ford Focus', 27000, 44],\n",
       "       ['Audi A4', 35000, 66]], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = df.values\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda Civic</td>\n",
       "      <td>22000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toyota Corolla</td>\n",
       "      <td>25000</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford Focus</td>\n",
       "      <td>27000</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audi A4</td>\n",
       "      <td>35000</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0      1   2\n",
       "0     Honda Civic  22000  22\n",
       "1  Toyota Corolla  25000  33\n",
       "2      Ford Focus  27000  44\n",
       "3         Audi A4  35000  66"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from azureml.core import Run\n",
    " \n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Honda Civic', 22],\n",
       "       ['Toyota Corolla', 33],\n",
       "       ['Ford Focus', 44],\n",
       "       ['Audi A4', 66]], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Model Training\n",
    "# X = data.drop(columns = 'deposit_cat')\n",
    "X = df.drop(columns = 'Price').values\n",
    "# y = df['Price'].values\n",
    "y = df['Price'].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15,\n",
    "random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=np.column_stack((X_train,Y_train))\n",
    "test=np.column_stack((X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Toyota Corolla', 33, 25000],\n",
       "       ['Honda Civic', 22, 22000],\n",
       "       ['Audi A4', 66, 35000]], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('object') and format specifier ('%.18e %.18e %.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-53c8c16e13e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# np.savetxt(args.test+\"/test.txt\",test,fmt=\"%f\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     raise TypeError(\"Mismatch between array dtype ('%s') and \"\n\u001b[1;32m   1425\u001b[0m                                     \u001b[0;34m\"format specifier ('%s')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                                     % (str(X.dtype), format))\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('object') and format specifier ('%.18e %.18e %.18e')"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"train.txt\",train)\n",
    "# np.savetxt(args.test+\"/test.txt\",test,fmt=\"%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
