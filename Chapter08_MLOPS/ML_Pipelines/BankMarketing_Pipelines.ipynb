{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    " \n",
    "from azureml.core import Dataset\n",
    " \n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    " \n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    " \n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    " \n",
    "from azureml.core.model import Model\n",
    " \n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize AzureML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Comput Target or use existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "aml_compute = \"automatecluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_compute = ComputeTarget(workspace=ws, name=aml_compute)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute, compute_config)\n",
    "\n",
    "aml_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training run's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "aml_run_config.environment.docker.base_image = \"mcr.microsoft.com/azureml/base:0.2.1\"\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-dataset-runtime[fuse]', 'packaging', 'numpy==1.16.2','azureml-sdk'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Datasets and Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Upload data to datastore and register datasets.\n",
    "# from azureml.core import Datastore\n",
    "# blob_datastore_name='kaggledatabook' # Name of the datastore to workspace\n",
    "# container_name=os.getenv(\"BLOB_CONTAINER\", \"opendata\") # Name of Azure blob container\n",
    "# account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"kaggledatabook\") # Storage account name\n",
    "# account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"QGmWeGNpXKFtmU7cnXW5Dg0LwX7L2SCbfjsZlBKKHHgsdhABgTfFo5Vh4ja3KTFdCfDrh7Q6n3SGpVlE4g/eXA==\") # Storage account access key\n",
    "\n",
    "# blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "#                                                          datastore_name=blob_datastore_name, \n",
    "#                                                          container_name=container_name, \n",
    "#                                                          account_name=account_name,\n",
    "#                                                          account_key=account_key)\n",
    "\n",
    "# blob_datastore.upload_files(files = ['./Bank.csv'], overwrite = True, show_progress = True)\n",
    "# from azureml.core.dataset import Dataset\n",
    "# bank_dataset = Dataset.Tabular.from_delimited_files(path=blob_datastore.path('Bank.csv'))\n",
    "# bank_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = Datastore.get(ws, \"kaggledatabook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "bank_dataset = Dataset.get_by_name(ws, name='bank_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct your pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Prep Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanse script is in /mnt/batch/tasks/shared/LS_root/mounts/clusters/adscompute/code/Users/prsing/BankMarketingAnalysis/ML_Pipelines/scripts/prepdata.\n",
      "cleansingStep created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# Define output after cleansing step\n",
    "cleansed_data = PipelineData(\"cleansed_data\", datastore=blob_datastore).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStep = PythonScriptStep(\n",
    "    name=\"Cleanse Bank Marketing Data\",\n",
    "    script_name=\"prep.py\", \n",
    "    arguments=[\"--output_cleanse\", cleansed_data],\n",
    "    inputs=[bank_dataset.as_named_input('bank_dataset')],\n",
    "    outputs=[cleansed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStep created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = PipelineData(\"model_file\", datastore=blob_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "trainmodel = PythonScriptStep(name=\"train_step\",\n",
    "                         script_name=\"./train.py\", \n",
    "                         arguments=['--learning_rate', 0.01, \n",
    "                                    '--n_estimators', 600, \n",
    "                                    '--max_depth', 9,\n",
    "                                    '--min_samples_split', 1200,\n",
    "                                    '--min_samples_leaf', 60,\n",
    "                                    '--subsample', 0.85,\n",
    "                                    '--random_state', 10,\n",
    "                                    '--max_features', 7,\n",
    "                                    '--model',model_file],\n",
    "                         inputs= [cleansed_data.parse_parquet_files(file_extension=None)],\n",
    "                         outputs=[model_file],                         \n",
    "                         compute_target=aml_compute, \n",
    "                         runconfig=aml_run_config,\n",
    "                         source_directory=train_model_folder,\n",
    "                         allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine steps and submit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [cleansingStep,trainmodel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Cleanse Bank Marketing Data [5d082efb][091249ef-40a6-4d2e-a29f-79c3d68a49ac], (This step is eligible to reuse a previous run's output)\n",
      "Created step train_step [f4cb3f05][03c8531d-4a30-46c1-8a16-de577be0a842], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 62f1d8fb-feb3-4824-bceb-3ff7d24cbad7\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Bank_Marketing/runs/62f1d8fb-feb3-4824-bceb-3ff7d24cbad7?wsid=/subscriptions/ab8f5415-63b3-4fd4-8a8a-9213316abb6e/resourcegroups/ADS_Book/workspaces/ADS_AMLworkspace\n"
     ]
    }
   ],
   "source": [
    "pipeline_run1 = Experiment(ws, 'Bank_Marketing').submit(pipeline1, regenerate_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Publishing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline1 = pipeline_run1.publish_pipeline(\n",
    "     name=\"BankMarketing_GB_Pipeline\",\n",
    "     description=\"Predict Term Deposit pipeline on bank marketing dataset\",\n",
    "     version=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3834a1a6-70ca-4146-b559-0665c88d5afe'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline1.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Schedule a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=5)\n",
    "recurring_schedule = Schedule.create(ws, name=\"bankmarketing_recurring5min\", \n",
    "                            description=\"Based on time\",\n",
    "                            pipeline_id=\"3834a1a6-70ca-4146-b559-0665c88d5afe\", \n",
    "                            experiment_name=\"Bank_Marketing\", \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deactivate the Pipeline schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(Name: bankmarketing_recurring5min,\n",
      "Id: 544dffab-e172-422f-a99e-9f7ae537b367,\n",
      "Status: Active,\n",
      "Pipeline Id: 3834a1a6-70ca-4146-b559-0665c88d5afe,\n",
      "Recurrence Details: Runs every 5 Minutes)\n"
     ]
    }
   ],
   "source": [
    "ss = Schedule.list(ws)\n",
    "for s in ss:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Pipeline Id</th><th>Recurrence Details</th></tr><tr><td>bankmarketing_recurring5min</td><td>544dffab-e172-422f-a99e-9f7ae537b367</td><td>Disabled</td><td><a href=\"https://ml.azure.com/pipelines/3834a1a6-70ca-4146-b559-0665c88d5afe?wsid=/subscriptions/ab8f5415-63b3-4fd4-8a8a-9213316abb6e/resourcegroups/ADS_Book/workspaces/ADS_AMLworkspace\" target=\"_blank\" rel=\"noopener\">3834a1a6-70ca-4146-b559-0665c88d5afe</a></td><td>Runs every 5 Minutes</td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: bankmarketing_recurring5min,\n",
       "Id: 544dffab-e172-422f-a99e-9f7ae537b367,\n",
       "Status: Disabled,\n",
       "Pipeline Id: 3834a1a6-70ca-4146-b559-0665c88d5afe,\n",
       "Recurrence Details: Runs every 5 Minutes)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_by_schedule_id(ws, schedule_id):\n",
    "    s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\n",
    "    s.disable()\n",
    "    return s\n",
    "\n",
    "stop_by_schedule_id(ws, \"544dffab-e172-422f-a99e-9f7ae537b367\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Schedule.list(ws)\n",
    "for s in ss:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
